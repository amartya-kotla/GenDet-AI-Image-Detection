{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLCzKlUkbaiw"
      },
      "source": [
        "# CLIP\n",
        "\n",
        "This section is a basic tutorial to use CLIP to generate the embeddings for your image dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDScCx77XcUD",
        "outputId": "1800d91f-191f-452f-ddb7-1a9de6efca4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m851.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-wbed_ccb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-wbed_ccb\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=f44fd46dae7675f9c98cf88489201cb3e12353442100355cd23b737ec898c7a6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ulhcymt/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tNuF0fjTiwoe",
        "outputId": "4c6f8f02-e98b-4fe0-e812-7db0e0e0e076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.5.1+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchviz)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchviz)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchviz)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchviz)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchviz)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchviz)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchviz-0.0.3\n"
          ]
        }
      ],
      "source": [
        "! pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCelMrTYXqMK"
      },
      "outputs": [],
      "source": [
        "import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "swkEMvhxYATj",
        "outputId": "d4e66eb7-7cdd-40bb-8e5f-6876ae41e98c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:40<00:00, 8.70MiB/s]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3ae9990e74e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ViT-B/32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load('ViT-B/32')\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9aPj-C5tY8s"
      },
      "outputs": [],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS9jzA4jYdV5",
        "outputId": "9bd71394-dcbc-46f1-c7da-179595f56daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 151,277,313\n"
          ]
        }
      ],
      "source": [
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKZeG3AwrXc6",
        "outputId": "35783549-5feb-42a1-e65a-fea3b201683f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 512])\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# img = Image.open(\"/content/drive/MyDrive/AmartyaPassportpic.jpg\")\n",
        "image_tensor = preprocess(Image.open(\"/content/drive/MyDrive/yourPic.jpg\")).unsqueeze(0).to(device)\n",
        "print(image_tensor.shape)\n",
        "image_embed = model.encode_image(image_tensor)\n",
        "print(image_embed.shape)\n",
        "print(image_embed.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "You can now use the above code and walk through your dataset directory and generate the embeddings.\n",
        "It is a time consuming process, and will require significant gpu usage, so you can run this part on Kaggle. Once done, save the embeddings to .pt files that you can use later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOMIkzfmyNQ5"
      },
      "source": [
        "# ESSENTIALS\n",
        "\n",
        "These are the building blocks of the modules used in the research paper, including the components for a transformer encoder block, the self attention, the custom loss functions defined in the paper. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V62kW4X_yQ__"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E50hEkJZyUph",
        "outputId": "f66b51aa-5528-4888-f600-f2e4627470f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c9j67URyYAx"
      },
      "outputs": [],
      "source": [
        "# Function to generate positional encodings for transformer\n",
        "\n",
        "def gen_pe(max_length, d_model, n=10000):\n",
        "\n",
        "  # generate an empty matrix for the positional encodings (pe)\n",
        "  pe = np.zeros(max_length*d_model, dtype=np.float32).reshape(max_length, d_model)\n",
        "\n",
        "  # for each position\n",
        "  for k in np.arange(max_length):\n",
        "\n",
        "    # for each dimension\n",
        "    for i in np.arange(d_model//2):\n",
        "\n",
        "      # calculate the internal value for sin and cos\n",
        "      theta = k / (n ** ((2*i)/d_model))\n",
        "\n",
        "      # even dims: sin\n",
        "      pe[k, 2*i] = math.sin(theta)\n",
        "\n",
        "      # odd dims: cos\n",
        "      pe[k, 2*i+1] = math.cos(theta)\n",
        "\n",
        "  pe = torch.tensor(pe)\n",
        "\n",
        "  return pe.view(1, max_length, d_model)\n",
        "\n",
        "pe = gen_pe(4, 196)\n",
        "pe = pe.to(device)\n",
        "# print(pe.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wCWVhpnybht"
      },
      "outputs": [],
      "source": [
        "# This class is used to transform the embeddings to 196 dimensional embeddings used in the transformer blocks defined in the research paper.\n",
        "# Sequence length is not given, so we use that as a hyperparameter seq_length. Reshaping the input vector such that it can be divided into a seq_length * 196 matrix\n",
        "\n",
        "\n",
        "class initial_data_transform(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, CLIP_embed_dim, seq_length):\n",
        "\n",
        "    super(initial_data_transform, self).__init__()\n",
        "\n",
        "    self.data_transform_weights = nn.Linear(CLIP_embed_dim, seq_length * embed_dim)\n",
        "    # self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.embed_dim = embed_dim\n",
        "    self.CLIP_embed_dim = CLIP_embed_dim\n",
        "\n",
        "\n",
        "  def forward(self, data):\n",
        "\n",
        "    data = self.data_transform_weights(data)\n",
        "    # print(data.shape)\n",
        "    data = data.view(data.size(0), self.seq_length, self.embed_dim)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIuLLnwmyhjJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, n_heads, embed_dim, seq_length) :\n",
        "\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.embed_dim = embed_dim\n",
        "    # self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.single_head_dim = embed_dim // n_heads\n",
        "\n",
        "    # self.data_transform_weights = nn.Linear(CLIP_embed_dim, seq_length * embed_dim)\n",
        "\n",
        "\n",
        "    # Generating the query, key and value vectors\n",
        "    self.query_weights = nn.Linear(embed_dim, embed_dim)\n",
        "    self.key_weights = nn.Linear(embed_dim, embed_dim)\n",
        "    self.value_weights = nn.Linear(embed_dim, embed_dim)\n",
        "    self.output_weights = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "\n",
        "  def generateMultiHeadQVK(self, data) :\n",
        "\n",
        "    # print(data.dtype)\n",
        "    Q = self.query_weights(data)\n",
        "    # print(Q.shape)\n",
        "    K = self.key_weights(data)\n",
        "    V = self.value_weights(data)\n",
        "\n",
        "  # Reshaping them such that we divide the Q,V,K vectors into unique Q,V,Ks for each attention head. \n",
        "\n",
        "    Q = Q.view(data.size(0), self.seq_length, self.n_heads, self.single_head_dim).transpose(1,2)\n",
        "    V = V.view(data.size(0), self.seq_length, self.n_heads, self.single_head_dim).transpose(1,2)\n",
        "    K = K.view(data.size(0), self.seq_length, self.n_heads, self.single_head_dim).transpose(1,2)\n",
        "\n",
        "\n",
        "    return Q,V,K\n",
        "\n",
        "\n",
        "\n",
        "  def generateAttentionScores(self, Q, K):\n",
        "\n",
        "    K = K.transpose(2,3)\n",
        "    # print(K.shape)\n",
        "    attention_matrix = torch.matmul(Q, K / math.sqrt(self.single_head_dim))\n",
        "    # print(attention_matrix.shape)\n",
        "    attention_scores = torch.softmax(attention_matrix, dim = -1)\n",
        "\n",
        "    return attention_scores\n",
        "\n",
        "\n",
        "  # Using the attention scores and calculating the new context rich vectors.\n",
        "  def getContextualizedVectors(self, attention_scores, V):\n",
        "\n",
        "    contextualized_vectors = torch.matmul(attention_scores, V)\n",
        "    batch_size = contextualized_vectors.size(0)\n",
        "    # print(contextualized_vectors.shape)\n",
        "    contextualized_vectors = contextualized_vectors.transpose(1,2).contiguous().view(batch_size, self.seq_length, self.embed_dim)\n",
        "    # print(contextualized_vectors.shape)\n",
        "    return contextualized_vectors\n",
        "\n",
        "\n",
        "  # The forward propagation pipeline in the attention mechanism\n",
        "  def forward(self, data):\n",
        "\n",
        "    # data = self.initial_data_transform(data)\n",
        "    upd_data = data + pe\n",
        "    # print(upd_data.shape)\n",
        "    Q, V, K = self.generateMultiHeadQVK(upd_data)\n",
        "    attention_scores = self.generateAttentionScores(Q, K)\n",
        "    contextualized_vectors = self.getContextualizedVectors(attention_scores, V)\n",
        "    multihead_output = self.output_weights(contextualized_vectors)\n",
        "    return multihead_output, data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m33Ttq4yk5R"
      },
      "outputs": [],
      "source": [
        "# The feed forward linear network after attention mechanism\n",
        "\n",
        "class Positionwise_Feed_Forward(nn.Module):\n",
        "   def __init__(self, embed_dim, ff_dim) :\n",
        "      super(Positionwise_Feed_Forward, self).__init__()\n",
        "\n",
        "      self.fc_layer1 = nn.Linear(embed_dim, ff_dim)\n",
        "      self.Relu = nn.ReLU()\n",
        "      self.fc_layer2 = nn.Linear(ff_dim, embed_dim)\n",
        "\n",
        "   def forward(self, multihead_output) :\n",
        "\n",
        "      fc1_output = self.fc_layer1(multihead_output)\n",
        "      # print(fc1_output.shape)\n",
        "      activation = self.Relu(fc1_output)\n",
        "      fc2_output = self.fc_layer2(activation)\n",
        "\n",
        "      return fc2_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hibc1sHmypP4"
      },
      "outputs": [],
      "source": [
        "# Adding the dropout and layer normalization in the end of the transformer block and constructing the transformer pipeline\n",
        "\n",
        "class GenDetTransformerBlock(nn.Module):\n",
        "\n",
        "   def __init__(self, seq_length, ff_dim = 4*196, embed_dim = 196, n_heads = 4,) :\n",
        "\n",
        "    super(GenDetTransformerBlock, self).__init__()\n",
        "\n",
        "\n",
        "    # self.MultiHeadAttention = MultiHeadAttention(n_heads = n_heads, batch_size = batch_size, embed_dim = embed_dim, seq_length = seq_length)\n",
        "    self.MultiHeadAttention = MultiHeadAttention(n_heads = n_heads, embed_dim = embed_dim, seq_length = seq_length)\n",
        "    self.Positionwise_Feed_Forward = Positionwise_Feed_Forward(embed_dim = embed_dim, ff_dim = ff_dim)\n",
        "    self.Norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.Norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "   def forward(self, data):\n",
        "      multihead_output, data = self.MultiHeadAttention(data)\n",
        "      add_norm1_output = (data + self.Norm1(self.dropout(multihead_output)))\n",
        "      ff_output = self.Positionwise_Feed_Forward(add_norm1_output)\n",
        "      final_add_norm2_output = (data + self.Norm2(self.dropout(ff_output)))\n",
        "      return final_add_norm2_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Functions\n",
        "\n",
        "These are the custom loss functions defined in the research paper for various stages of the training and testing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e0VLDReyyDq"
      },
      "outputs": [],
      "source": [
        "from torch import linalg as LA\n",
        "\n",
        "class TeacherStudent_loss_fake(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(TeacherStudent_loss_fake, self).__init__()\n",
        "\n",
        "    self.reg_M = 0.5\n",
        "\n",
        "  def forward(self, y_t, y_s):\n",
        "      y_t_Norm = LA.norm(y_t, dim=1, keepdim=True)\n",
        "      # print(y_t_Norm.shape)\n",
        "      Y_t = y_t / y_t_Norm\n",
        "      # print(Y_t.shape)\n",
        "\n",
        "\n",
        "      y_s_Norm = LA.norm(y_s, dim=1, keepdim=True)\n",
        "      # print(y_s_Norm.shape)\n",
        "      Y_s = y_s / y_s_Norm\n",
        "      # print(Y_s.shape)\n",
        "\n",
        "      discrep = (Y_t- Y_s).pow(2).sum(dim=1)\n",
        "      losses = torch.clamp(self.reg_M - discrep, min=0)\n",
        "      # print(losses.shape, losses)\n",
        "      batch_loss = losses.mean()\n",
        "      return batch_loss\n",
        "      # return torch.max(t)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kwoetZrzoez"
      },
      "outputs": [],
      "source": [
        "class mse_loss_new(nn.Module):\n",
        "    # Sums over non-batch dimensions, then averages over the batch dimension.\n",
        "    def __init__(self):\n",
        "      super(mse_loss_new, self).__init__()\n",
        "\n",
        "    def forward(self, z_t, z_s):\n",
        "      # losses = (z_t - z_s).pow(2).mean(dim=list(range(1, z_t.ndim)))\n",
        "      diff = (z_t - z_s).pow(2)\n",
        "      # print(diff)\n",
        "      # print(torch.min(diff), torch.max(diff))\n",
        "      losses = diff.mean(dim=list(range(1, z_t.ndim)))\n",
        "      # print(losses)\n",
        "      return losses.mean()\n",
        "\n",
        "mse_new = mse_loss_new()\n",
        "# loss = mse_new(arr1, arr2)\n",
        "# print(loss)\n",
        "# loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUzb4sbeRJNr"
      },
      "outputs": [],
      "source": [
        "class mse_loss_new_withoutBatchAverage(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super(mse_loss_new_withoutBatchAverage, self).__init__()\n",
        "\n",
        "    def forward(self, z_t, z_s):\n",
        "      # losses = (z_t - z_s).pow(2).mean(dim=list(range(1, z_t.ndim)))\n",
        "      diff = (z_t - z_s).pow(2)\n",
        "      # print(diff)\n",
        "      # print(torch.min(diff), torch.max(diff))\n",
        "      losses = diff.mean(dim=list(range(1, z_t.ndim))).view(diff.size(0), 1)\n",
        "      # print(losses)\n",
        "      return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Putting the GenDet Model together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1CEJBBAzvJ3"
      },
      "outputs": [],
      "source": [
        "# Adding the teacher, student and augmenter modules seperately, and a sigmoid layer to get the outputs.\n",
        "\n",
        "class GenDetComplete(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, seq_length, CLIP_embed_dim, n_heads, ff_dim):\n",
        "\n",
        "    super(GenDetComplete, self).__init__()\n",
        "\n",
        "    self.initial_data_transform = initial_data_transform(embed_dim, CLIP_embed_dim, seq_length)\n",
        "    self.Teacher = GenDetTransformerBlock(seq_length, ff_dim, embed_dim, n_heads)\n",
        "    self.Student = GenDetTransformerBlock(seq_length, ff_dim, embed_dim, n_heads)\n",
        "    self.Augmenter = GenDetTransformerBlock(seq_length, ff_dim, embed_dim, n_heads)\n",
        "    self.fc_teacher = nn.Linear(seq_length * embed_dim, 1)\n",
        "    # self.fc_student = nn.Linear(seq_length * embed_dim, 1)\n",
        "    self.sigmoid_teacher = nn.Sigmoid()\n",
        "    self.sigmoid_student = nn.Sigmoid()\n",
        "    # self.batch_size = batch_size\n",
        "    # self.fc_final = nn.Linear(1,1)\n",
        "    # self.sigmoid_final = nn.Sigmoid()\n",
        "    # self.MSELoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, data, mode):\n",
        "\n",
        "    data = self.initial_data_transform(data)\n",
        "    batch_size = data.size(0)\n",
        "\n",
        "    # if mode == \"GenDet\":\n",
        "    #   data = self.Augmenter(data)\n",
        "\n",
        "\n",
        "  # Added conditions to ignore the student and gendet blocks during the teacher stage of training\n",
        "\n",
        "    if mode == \"Inference\":\n",
        "      stu_out = self.Student(data)\n",
        "      stu_out = stu_out.contiguous().view(batch_size, 784)\n",
        "\n",
        "    elif mode != \"Teacher\":\n",
        "\n",
        "      data = self.Augmenter(data)\n",
        "\n",
        "      stu_out = self.Student(data)\n",
        "      stu_out = stu_out.contiguous().view(batch_size, 784)\n",
        "\n",
        "\n",
        "    teach_out = self.Teacher(data)\n",
        "    teach_out = teach_out.contiguous().view(batch_size, 784)\n",
        "    teach_out_final = self.fc_teacher(teach_out)\n",
        "    # print(teach_out_final.shape)\n",
        "    teach_out_final = self.sigmoid_teacher(teach_out_final)\n",
        "\n",
        "    if mode != \"Teacher\":\n",
        "      return teach_out, stu_out\n",
        "\n",
        "\n",
        "    return teach_out_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiXi4PKaz07m"
      },
      "outputs": [],
      "source": [
        "# The Binary Classifier used for testing with the loss function as the input to the neural network. \n",
        "\n",
        "class Binary_Classifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(Binary_Classifier,self).__init__()\n",
        "\n",
        "    self.MSELoss = nn.MSELoss()\n",
        "    self.fc_final = nn.Linear(1,1)\n",
        "    self.sigmoid_final = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, teach_out, stu_out):\n",
        "\n",
        "    loss = self.MSELoss(teach_out, stu_out).view(1,1)\n",
        "    print(loss)\n",
        "    p_out = self.fc_final(loss)\n",
        "    out = self.sigmoid_final(p_out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_aPhDHRcGrf"
      },
      "source": [
        "# Remove parameter settings out of model and put in training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugkOiQ4Dnict"
      },
      "outputs": [],
      "source": [
        "#mse loss calculated in which dimension, with or without mean? also BATCH_SIZE FOR LOSSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJcE9LXIA7_M"
      },
      "source": [
        "#  MODEL AND DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdu_Vm_esZ8B",
        "outputId": "5d2ab563-2bec-46ac-b001-581df05b016c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12068, 512])\n",
            "torch.Size([13330, 512])\n",
            "torch.Size([25398, 512])\n",
            "torch.Size([12068, 1]) torch.Size([13330, 1]) torch.Size([25398, 1])\n",
            "tensor([0.])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-176-4caa8d30e739>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  real_tensors = torch.load(\"/content/drive/MyDrive/gendet_image_encodings/image_encodings/real_image_encodings\")\n",
            "<ipython-input-176-4caa8d30e739>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  fake_tensors = torch.load(\"/content/drive/MyDrive/gendet_image_encodings/image_encodings/fake_image_encodings.pt\")\n"
          ]
        }
      ],
      "source": [
        "# Importing the .pt files mentioned in the CLIP section, for the real and fake image encodings.\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "real_tensors = torch.load(\"/content/drive/MyDrive/gendet_image_encodings/image_encodings/real_image_encodings\")\n",
        "fake_tensors = torch.load(\"/content/drive/MyDrive/gendet_image_encodings/image_encodings/fake_image_encodings.pt\")\n",
        "all_tensors = real_tensors + fake_tensors\n",
        "# print(len(real_tensors))\n",
        "real_tensors = torch.cat(real_tensors)\n",
        "fake_tensors = torch.cat(fake_tensors)\n",
        "all_tensors = torch.cat(all_tensors)\n",
        "print(real_tensors.shape)\n",
        "print(fake_tensors.shape)\n",
        "print(all_tensors.shape)\n",
        "\n",
        "real_outputs = torch.ones((len(real_tensors), 1), dtype=torch.float32)\n",
        "fake_outputs = torch.zeros((len(fake_tensors), 1), dtype=torch.float32)\n",
        "\n",
        "all_outputs = torch.cat([real_outputs, fake_outputs])\n",
        "\n",
        "print(real_outputs.shape, fake_outputs.shape, all_outputs.shape)\n",
        "print(all_outputs[12068])\n",
        "\n",
        "real_dataset = TensorDataset(real_tensors)\n",
        "fake_dataset = TensorDataset(fake_tensors)\n",
        "full_dataset = TensorDataset(all_tensors, all_outputs)\n",
        "\n",
        "train_split = 0.8\n",
        "val_split = 0.2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#---------------Creating separeate datasets for real, fake and all images as intended in the research paper---------------------\n",
        "\n",
        "real_dataset_train, real_dataset_val = random_split(real_dataset, [math.ceil(train_split * len(real_dataset)), math.floor(val_split * len(real_dataset))])\n",
        "\n",
        "# print(len(real_dataset_train.indices), len(real_dataset_val.indices))\n",
        "\n",
        "fake_dataset_train, fake_dataset_val = random_split(fake_dataset, [math.ceil(train_split * len(fake_dataset)), math.floor(val_split * len(fake_dataset))])\n",
        "\n",
        "# print(len(fake_dataset_train.indices), len(fake_dataset_val.indices))\n",
        "\n",
        "full_dataset_train, full_dataset_val = random_split(full_dataset, [math.ceil(train_split * len(full_dataset)), math.floor(val_split * len(full_dataset))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OEqsMC4lxBsk",
        "outputId": "06cecc21-478b-4b1d-e357-546ac8427e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "teacherLoaderTrain = DataLoader(full_dataset_train, batch_size=32, shuffle=True)\n",
        "teacherLoaderVal = DataLoader(full_dataset_val, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "for x, y in teacherLoaderTrain:\n",
        "  print(x.shape, y.shape)\n",
        "  break\n",
        "\n",
        "for x, y in teacherLoaderVal:\n",
        "  print(x.shape, y.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pTsIV_Rl4rr4",
        "outputId": "4172a651-1016-4ae2-8323-26e3d97e8323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 512]) 0\n",
            "torch.Size([64, 512])\n"
          ]
        }
      ],
      "source": [
        "realLoaderTrain = DataLoader(real_dataset_train, batch_size=64, shuffle=True)\n",
        "realLoaderVal = DataLoader(real_dataset_val, batch_size=64, shuffle=True)\n",
        "\n",
        "for i,x in enumerate(realLoaderTrain):\n",
        "  print(x[0].shape, i)\n",
        "  x[0].to\n",
        "  break\n",
        "\n",
        "for x in realLoaderVal:\n",
        "  print(x[0].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91X68-LkmJbO",
        "outputId": "d318917b-f050-4fd0-b360-7724dbe0bdbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 512])\n",
            "torch.Size([64, 512])\n"
          ]
        }
      ],
      "source": [
        "fakeLoaderTrain = DataLoader(fake_dataset_train, batch_size=64, shuffle=True)\n",
        "fakeLoaderVal = DataLoader(fake_dataset_val, batch_size=64, shuffle=True)\n",
        "\n",
        "for x in fakeLoaderTrain:\n",
        "  print(x[0].shape)\n",
        "  break\n",
        "\n",
        "for x in fakeLoaderVal:\n",
        "  print(x[0].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZQlvYUmQ5IR"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter values\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "embed_dim = 196\n",
        "batch_size = 1\n",
        "seq_length = 4\n",
        "ff_dim = 196*4\n",
        "n_heads = 4\n",
        "ClIP_embed_dim = 512\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "lr_transformer = 0.00001\n",
        "\n",
        "lr_gendet = 0.001\n",
        "\n",
        "# optimizer = optim.AdamW(genDetComplete.parameters(), lr= lr_transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function to freeze student and augmenter parameters based on training mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH4RRrCdVtu8"
      },
      "outputs": [],
      "source": [
        "def SetTrainingModeParams(mode:str):\n",
        "\n",
        "    if mode == \"Teacher\":\n",
        "      for param in model.Teacher.parameters():\n",
        "        param.requires_grad =  True\n",
        "\n",
        "      for param in model.Student.parameters():\n",
        "        param.requires_grad =  False\n",
        "\n",
        "      for param in model.Augmenter.parameters():\n",
        "        param.requires_grad =  False\n",
        "\n",
        "    elif mode == \"Real\" or mode == \"Fake\":\n",
        "      for param in model.Student.parameters():\n",
        "        param.requires_grad =  True\n",
        "\n",
        "      for param in model.Teacher.parameters():\n",
        "        param.requires_grad =  False\n",
        "\n",
        "      for param in model.Augmenter.parameters():\n",
        "        param.requires_grad =  False\n",
        "\n",
        "    elif mode == \"Fake Augmenter\":\n",
        "      for param in model.Augmenter.parameters():\n",
        "        param.requires_grad =  True\n",
        "\n",
        "      for param in model.Student.parameters():\n",
        "        param.requires_grad =  False\n",
        "\n",
        "      for param in model.Teacher.parameters():\n",
        "        param.requires_grad =  False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RtDDaOUMTOu"
      },
      "outputs": [],
      "source": [
        "modes = [\"Real\",\"Fake\",\"Fake Augmenter\"]\n",
        "losses = {}\n",
        "\n",
        "\n",
        "#real images\n",
        "losses[\"Teacher\"] = nn.BCELoss()\n",
        "losses[\"Real\"] = mse_loss_new()\n",
        "losses[\"Fake\"] = TeacherStudent_loss_fake()\n",
        "losses[\"Fake Augmenter\"] = mse_loss_new()\n",
        "\n",
        "#fake images\n",
        "# losses[\"Teacher\"] = nn.BCELoss()\n",
        "# losses[\"Student\"] = TeacherStudent_loss_fake()\n",
        "# losses[\"GenDet\"] = nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqRB4l7iswlQ"
      },
      "source": [
        "# Teacher Pretraining Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObZXY1wmnrG-"
      },
      "outputs": [],
      "source": [
        "# Training and Inference methods\n",
        "# Pass Parameters?\n",
        "#  NOT PASSING FAKE DATASET IN THREE STAGE\n",
        "def Teacher_PreTrainer(trainLoader, valLoader):\n",
        "\n",
        "  SetTrainingModeParams(\"Teacher\")\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = lr_transformer)\n",
        "\n",
        "\n",
        "  loss_fn = losses[\"Teacher\"]\n",
        "\n",
        "  train_losses = []\n",
        "  train_accuracy =[]\n",
        "\n",
        "  val_losses = []\n",
        "  val_accuracy =[]\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_train_loss = 0\n",
        "    epoch_train_accuracy = []\n",
        "\n",
        "    epoch_val_loss = 0\n",
        "    epoch_val_accuracy = []\n",
        "    model.train()\n",
        "\n",
        "    print('EPOCH {}:\\n'.format(epoch + 1))\n",
        "    for x, y in trainLoader:\n",
        "\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # print(x.shape, y.shape)\n",
        "      optimizer.zero_grad()\n",
        "      teach_out_final = model(x, \"Teacher\")\n",
        "      loss = loss_fn(teach_out_final, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_train_loss += loss.item()\n",
        "      # print(epoch_train_loss)\n",
        "\n",
        "      # print(epoch_train_loss)\n",
        "\n",
        "      teach_out_final = (teach_out_final > 0.5).float()\n",
        "      correct_predicts = (teach_out_final == y).float()\n",
        "      accuracy = ((correct_predicts.sum() / correct_predicts.size(0)) * 100)\n",
        "      epoch_train_accuracy.append(accuracy)\n",
        "\n",
        "\n",
        "    print('TRAIN_LOSS: {}'.format(epoch_train_loss))\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    avg_accuracy = (torch.stack(epoch_train_accuracy)).mean()\n",
        "    train_accuracy.append(avg_accuracy)\n",
        "    print('TRAIN_ACC: {}\\n'.format(avg_accuracy))\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for x, y in valLoader:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        outs = model(x, \"Teacher\")\n",
        "        loss = loss_fn(outs, y)\n",
        "\n",
        "        epoch_val_loss += loss.item()\n",
        "\n",
        "        outs = (outs > 0.5).float()\n",
        "        outs = (outs == y).float()\n",
        "        accuracy = ((outs.sum() / outs.size(0)) * 100)\n",
        "        epoch_val_accuracy.append(accuracy)\n",
        "\n",
        "    print('VAL_LOSS: {}'.format(epoch_val_loss))\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    avg_accuracy = (torch.stack(epoch_val_accuracy)).mean()\n",
        "    print('VAL_ACC: {}\\n'.format(avg_accuracy))\n",
        "    val_accuracy.append(epoch_val_accuracy)\n",
        "\n",
        "  return train_accuracy, train_losses, val_accuracy, val_losses\n",
        "\n",
        "#Warmup steps?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VoyVAHNN3FDJ",
        "outputId": "dc2590b5-fddd-40ce-b568-7d25754d7ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "\n",
            "TRAIN_LOSS: 265.6397636681795\n",
            "TRAIN_ACC: 81.4212875366211\n",
            "\n",
            "VAL_LOSS: 41.93561540171504\n",
            "VAL_ACC: 88.81681823730469\n",
            "\n",
            "EPOCH 2:\n",
            "\n",
            "TRAIN_LOSS: 163.51772906258702\n",
            "TRAIN_ACC: 89.52247619628906\n",
            "\n",
            "VAL_LOSS: 41.593817837536335\n",
            "VAL_ACC: 90.36180114746094\n",
            "\n",
            "EPOCH 3:\n",
            "\n",
            "TRAIN_LOSS: 143.82712575793266\n",
            "TRAIN_ACC: 90.93440246582031\n",
            "\n",
            "VAL_LOSS: 34.26261672284454\n",
            "VAL_ACC: 91.89910125732422\n",
            "\n",
            "EPOCH 4:\n",
            "\n",
            "TRAIN_LOSS: 135.09545893594623\n",
            "TRAIN_ACC: 91.5343246459961\n",
            "\n",
            "VAL_LOSS: 32.24607868865132\n",
            "VAL_ACC: 92.07170867919922\n",
            "\n",
            "EPOCH 5:\n",
            "\n",
            "TRAIN_LOSS: 126.94067922234535\n",
            "TRAIN_ACC: 92.16043090820312\n",
            "\n",
            "VAL_LOSS: 33.17227216344327\n",
            "VAL_ACC: 92.11101531982422\n",
            "\n",
            "EPOCH 6:\n",
            "\n",
            "TRAIN_LOSS: 123.63036894239485\n",
            "TRAIN_ACC: 92.34236145019531\n",
            "\n",
            "VAL_LOSS: 31.92129190824926\n",
            "VAL_ACC: 92.45709991455078\n",
            "\n",
            "EPOCH 7:\n",
            "\n",
            "TRAIN_LOSS: 121.38128453306854\n",
            "TRAIN_ACC: 92.44570922851562\n",
            "\n",
            "VAL_LOSS: 31.221492186188698\n",
            "VAL_ACC: 92.73995208740234\n",
            "\n",
            "EPOCH 8:\n",
            "\n",
            "TRAIN_LOSS: 117.36820307001472\n",
            "TRAIN_ACC: 92.72589874267578\n",
            "\n",
            "VAL_LOSS: 32.608694788999856\n",
            "VAL_ACC: 92.53572082519531\n",
            "\n",
            "EPOCH 9:\n",
            "\n",
            "TRAIN_LOSS: 115.85206874459982\n",
            "TRAIN_ACC: 92.81956481933594\n",
            "\n",
            "VAL_LOSS: 29.301667038351297\n",
            "VAL_ACC: 93.2039566040039\n",
            "\n",
            "EPOCH 10:\n",
            "\n",
            "TRAIN_LOSS: 113.95675650052726\n",
            "TRAIN_ACC: 92.88353729248047\n",
            "\n",
            "VAL_LOSS: 29.880338294431567\n",
            "VAL_ACC: 92.92452239990234\n",
            "\n",
            "EPOCH 11:\n",
            "\n",
            "TRAIN_LOSS: 111.89571738056839\n",
            "TRAIN_ACC: 93.17388916015625\n",
            "\n",
            "VAL_LOSS: 28.70988953113556\n",
            "VAL_ACC: 93.11337280273438\n",
            "\n",
            "EPOCH 12:\n",
            "\n",
            "TRAIN_LOSS: 110.88180174306035\n",
            "TRAIN_ACC: 93.03101348876953\n",
            "\n",
            "VAL_LOSS: 30.588415143080056\n",
            "VAL_ACC: 92.83052825927734\n",
            "\n",
            "EPOCH 13:\n",
            "\n",
            "TRAIN_LOSS: 109.14035102911294\n",
            "TRAIN_ACC: 93.24295043945312\n",
            "\n",
            "VAL_LOSS: 28.84009088203311\n",
            "VAL_ACC: 92.91683197021484\n",
            "\n",
            "EPOCH 14:\n",
            "\n",
            "TRAIN_LOSS: 107.17229505255818\n",
            "TRAIN_ACC: 93.3559799194336\n",
            "\n",
            "VAL_LOSS: 30.33944906387478\n",
            "VAL_ACC: 93.11337280273438\n",
            "\n",
            "EPOCH 15:\n",
            "\n",
            "TRAIN_LOSS: 104.6471360353753\n",
            "TRAIN_ACC: 93.50885772705078\n",
            "\n",
            "VAL_LOSS: 28.68938394356519\n",
            "VAL_ACC: 93.57738494873047\n",
            "\n",
            "EPOCH 16:\n",
            "\n",
            "TRAIN_LOSS: 103.0178152769804\n",
            "TRAIN_ACC: 93.52838134765625\n",
            "\n",
            "VAL_LOSS: 30.324699106626213\n",
            "VAL_ACC: 93.1723403930664\n",
            "\n",
            "EPOCH 17:\n",
            "\n",
            "TRAIN_LOSS: 100.38976982608438\n",
            "TRAIN_ACC: 93.86779022216797\n",
            "\n",
            "VAL_LOSS: 28.594031773507595\n",
            "VAL_ACC: 93.52268981933594\n",
            "\n",
            "EPOCH 18:\n",
            "\n",
            "TRAIN_LOSS: 98.64355889800936\n",
            "TRAIN_ACC: 93.94637298583984\n",
            "\n",
            "VAL_LOSS: 27.87950689252466\n",
            "VAL_ACC: 93.66368865966797\n",
            "\n",
            "EPOCH 19:\n",
            "\n",
            "TRAIN_LOSS: 95.80287798307836\n",
            "TRAIN_ACC: 94.23704528808594\n",
            "\n",
            "VAL_LOSS: 27.92697460670024\n",
            "VAL_ACC: 94.02942657470703\n",
            "\n",
            "EPOCH 20:\n",
            "\n",
            "TRAIN_LOSS: 93.09473804756999\n",
            "TRAIN_ACC: 94.26165008544922\n",
            "\n",
            "VAL_LOSS: 28.991336847655475\n",
            "VAL_ACC: 93.8209228515625\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "# lr_transformer = 0.000001\n",
        "lr_transformer = 0.00001\n",
        "Teacher_PreTrainer(teacherLoaderTrain, teacherLoaderVal)\n",
        "\n",
        "# torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bvQSQndY-MS"
      },
      "source": [
        "# Load model again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cZYROeTklsR9"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/teacher_pretrained.pt'\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "\n",
        "model = GenDetComplete(embed_dim=embed_dim, seq_length=seq_length, ff_dim=ff_dim, n_heads=n_heads, CLIP_embed_dim=ClIP_embed_dim)\n",
        "\n",
        "# , map_location=torch.device(\"cpu\")\n",
        "\n",
        "model.load_state_dict(torch.load(PATH, weights_only=False))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# BC = Binary_Classifier().to(device)\n",
        "\n",
        "# device\n",
        "# epochs = 20\n",
        "# lr_transformer = 0.000001\n",
        "# # lr_transformer = 0.00001\n",
        "# Teacher_PreTrainer(teacherLoaderTrain, teacherLoaderVal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEmnbQGArQ3h"
      },
      "source": [
        "# Three stage model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PLVr7aIGvlkD",
        "outputId": "81f1218b-f1dd-4ec5-b446-dda2699c13b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "\n",
            "Real TRAIN LOSS: 49.657793790102005\n",
            "Real VAL LOSS: 1.413122996687889\n",
            "Fake TRAIN LOSS: 21.003014595189597\n",
            "Fake VAL LOSS: 0.0013384358026087284\n",
            "Fake Augmenter TRAIN LOSS: 153.37348747253418\n",
            "Fake Augmenter VAL LOSS: 4.969570718705654\n",
            "EPOCH 2:\n",
            "\n",
            "Real TRAIN LOSS: 48.71669578552246\n",
            "Real VAL LOSS: 1.4312446527183056\n",
            "Fake TRAIN LOSS: 21.925677185878158\n",
            "Fake VAL LOSS: 0.0023814779706299305\n",
            "Fake Augmenter TRAIN LOSS: 222.89969128370285\n",
            "Fake Augmenter VAL LOSS: 20.1786727309227\n",
            "EPOCH 3:\n",
            "\n",
            "Real TRAIN LOSS: 79.6107913851738\n",
            "Real VAL LOSS: 3.809926487505436\n",
            "Fake TRAIN LOSS: 10.216692567337304\n",
            "Fake VAL LOSS: 0.0\n",
            "Fake Augmenter TRAIN LOSS: 125.79384195804596\n",
            "Fake Augmenter VAL LOSS: 4.22846432775259\n",
            "EPOCH 4:\n",
            "\n",
            "Real TRAIN LOSS: 59.524302273988724\n",
            "Real VAL LOSS: 1.5210529416799545\n",
            "Fake TRAIN LOSS: 11.675844843965024\n",
            "Fake VAL LOSS: 0.00038497941568493843\n",
            "Fake Augmenter TRAIN LOSS: 121.27310118079185\n",
            "Fake Augmenter VAL LOSS: 6.534250348806381\n",
            "EPOCH 5:\n",
            "\n",
            "Real TRAIN LOSS: 48.05335921049118\n",
            "Real VAL LOSS: 1.2510942481458187\n",
            "Fake TRAIN LOSS: 27.4143352504816\n",
            "Fake VAL LOSS: 0.004294569662306458\n",
            "Fake Augmenter TRAIN LOSS: 143.80088818073273\n",
            "Fake Augmenter VAL LOSS: 4.672000043094158\n"
          ]
        }
      ],
      "source": [
        "#try 10?\n",
        "def ThreeStage_Trainer(realLoaderTrain, realLoaderVal, fakeLoaderTrain, fakeLoaderVal):\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), lr = lr_transformer)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      print('EPOCH {}:\\n'.format(epoch + 1))\n",
        "      for mode in modes:\n",
        "\n",
        "        SetTrainingModeParams(mode)\n",
        "\n",
        "        # We set the loss function to use depending on which phase of training it is.\n",
        "        loss_fn = losses[mode]\n",
        "        train_loss = 0\n",
        "        train_accuracy = []\n",
        "\n",
        "        val_loss = 0\n",
        "        val_accuracy = []\n",
        "\n",
        "\n",
        "        if mode == \"Real\":\n",
        "          model.train()\n",
        "          for x in realLoaderTrain:\n",
        "\n",
        "            X = x[0].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            teach_out, stud_out = model(X, mode)\n",
        "            # print(teach_out.shape)\n",
        "            loss = loss_fn(teach_out, stud_out)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            # print(train_loss)\n",
        "\n",
        "          print('{mode} TRAIN LOSS: {train_loss}'.format(mode = mode, train_loss = train_loss))\n",
        "\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "\n",
        "            for x in realLoaderVal:\n",
        "\n",
        "              X = x[0].to(device)\n",
        "\n",
        "              teach_out, stud_out = model(X, mode)\n",
        "              loss = loss_fn(teach_out, stud_out)\n",
        "\n",
        "              val_loss += loss.item()\n",
        "\n",
        "          print('{mode} VAL LOSS: {val_loss}'.format(mode = mode, val_loss = val_loss))\n",
        "\n",
        "        else:\n",
        "          model.train()\n",
        "          for x in fakeLoaderTrain:\n",
        "\n",
        "            X = x[0].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            teach_out, stud_out = model(X, mode)\n",
        "            loss = loss_fn(teach_out, stud_out)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "          print('{mode} TRAIN LOSS: {train_loss}'.format(mode = mode, train_loss = train_loss))\n",
        "\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "\n",
        "            for x in fakeLoaderVal:\n",
        "\n",
        "              X = x[0].to(device)\n",
        "\n",
        "              teach_out, stud_out = model(X, mode)\n",
        "              loss = loss_fn(teach_out, stud_out)\n",
        "\n",
        "              val_loss += loss.item()\n",
        "\n",
        "          print('{mode} VAL LOSS: {val_loss}'.format(mode = mode, val_loss = val_loss))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "lr_transformer = 0.0001\n",
        "ThreeStage_Trainer(realLoaderTrain, realLoaderVal, fakeLoaderTrain, fakeLoaderVal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5hQywM2IB9h"
      },
      "source": [
        "# Making dataset for the Binary Classifier using the GenDet outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyPic6a35kIX",
        "outputId": "7975ebd7-ccd9-4332-8691-e59290d5bbd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "635 635\n",
            "torch.Size([32, 784])\n",
            "torch.Size([32, 784])\n",
            "159 159\n",
            "torch.Size([32, 784])\n",
            "torch.Size([32, 784])\n",
            "635 159\n",
            "torch.Size([32, 1]) torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "# Creating Classifier data using model inference\n",
        "\n",
        "GenDetTeachOuts_train = []\n",
        "GenDetStudOuts_train = []\n",
        "GenDetTeachOuts_val = []\n",
        "GenDetStudOuts_val = []\n",
        "\n",
        "GenDetActualOuts_train = []\n",
        "GenDetActualOuts_val = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for x, y in teacherLoaderTrain:\n",
        "\n",
        "    x = x.to(device)\n",
        "    # y = y.to(device)\n",
        "    teach_out, stud_out = model(x, \"Inference\")\n",
        "    GenDetTeachOuts_train.append(teach_out)\n",
        "    GenDetStudOuts_train.append(stud_out)\n",
        "    GenDetActualOuts_train.append(y)\n",
        "\n",
        "  for x, y in teacherLoaderVal:\n",
        "\n",
        "    x = x.to(device)\n",
        "    # y = y.to(device)\n",
        "    teach_out, stud_out = model(x, \"Inference\")\n",
        "    GenDetTeachOuts_val.append(teach_out)\n",
        "    GenDetStudOuts_val.append(stud_out)\n",
        "    GenDetActualOuts_val.append(y)\n",
        "\n",
        "\n",
        "print(len(GenDetTeachOuts_train), len(GenDetStudOuts_train))\n",
        "print(GenDetTeachOuts_train[0].shape)\n",
        "print(GenDetStudOuts_train[0].shape)\n",
        "\n",
        "print(len(GenDetTeachOuts_val), len(GenDetStudOuts_val))\n",
        "print(GenDetTeachOuts_val[0].shape)\n",
        "print(GenDetStudOuts_val[0].shape)\n",
        "\n",
        "print(len(GenDetActualOuts_train), len(GenDetActualOuts_val))\n",
        "print(GenDetActualOuts_train[0].shape, GenDetActualOuts_val[0].shape)\n",
        "\n",
        "# BC_tensors = torch.cat(GenDetOuts)\n",
        "# print(BC_tensors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeiA3_Ec-tFf",
        "outputId": "b3a5e831-1188-4c7c-db06-456e7f564c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20319, 784]) torch.Size([5079, 784]) torch.Size([20319, 784]) torch.Size([5079, 784]) torch.Size([20319, 1]) torch.Size([5079, 1])\n"
          ]
        }
      ],
      "source": [
        "GenDetTeachOuts_train = torch.cat(GenDetTeachOuts_train)\n",
        "GenDetTeachOuts_val = torch.cat(GenDetTeachOuts_val)\n",
        "\n",
        "GenDetStudOuts_train = torch.cat(GenDetStudOuts_train)\n",
        "GenDetStudOuts_val = torch.cat(GenDetStudOuts_val)\n",
        "\n",
        "GenDetActualOuts_train = torch.cat(GenDetActualOuts_train)\n",
        "GenDetActualOuts_val = torch.cat(GenDetActualOuts_val)\n",
        "\n",
        "\n",
        "print(GenDetTeachOuts_train.shape, GenDetTeachOuts_val.shape,\n",
        "      GenDetStudOuts_train.shape, GenDetStudOuts_val.shape,\n",
        "      GenDetActualOuts_train.shape, GenDetActualOuts_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn85bbP291OQ"
      },
      "outputs": [],
      "source": [
        "# The train and validation datasets for the Binary Classifier\n",
        "\n",
        "\n",
        "BCDataset_train = TensorDataset(GenDetTeachOuts_train, GenDetStudOuts_train, GenDetActualOuts_train)\n",
        "BCDataset_val = TensorDataset(GenDetTeachOuts_val, GenDetStudOuts_val, GenDetActualOuts_val)\n",
        "\n",
        "BCTrainLoader = DataLoader(BCDataset_train, batch_size=32, shuffle=True)\n",
        "BCValLoader = DataLoader(BCDataset_val, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe2r9RxCEDlu",
        "outputId": "731a703b-853b-40d8-ac3f-f187f9eace46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 784]) torch.Size([32, 784]) torch.Size([32, 1])\n",
            "torch.Size([32, 784]) torch.Size([32, 784]) torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "for x, y, z in BCTrainLoader:\n",
        "  print(x.shape, y.shape, z.shape)\n",
        "  break\n",
        "\n",
        "for x, y, z in BCValLoader:\n",
        "  print(x.shape, y.shape, z.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwDlIzytPaid"
      },
      "source": [
        "# Training the Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhggRgtOjsUS"
      },
      "outputs": [],
      "source": [
        "# BC = Binary_Classifier().to(device)\n",
        "class FinalClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FinalClassifier, self).__init__()\n",
        "\n",
        "\n",
        "    self.loss_fn = mse_loss_new_withoutBatchAverage()\n",
        "    self.fc1 = nn.Linear(1,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, teach_out, stud_out):\n",
        "    loss = self.loss_fn(teach_out, stud_out)\n",
        "    fc1_out = self.fc1(loss)\n",
        "    outs = self.sigmoid(fc1_out)\n",
        "\n",
        "    return outs\n",
        "\n",
        "FC = FinalClassifier().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ows20KiwRmr-"
      },
      "outputs": [],
      "source": [
        "def BinaryClassifier_Trainer(dataLoaderTrain, dataLoaderVal):\n",
        "\n",
        "  optimizer = optim.AdamW(FC.parameters(), lr = lr_transformer)\n",
        "  loss_fn = nn.BCELoss()\n",
        "\n",
        "  train_losses = []\n",
        "  train_accuracy =[]\n",
        "\n",
        "  val_losses = []\n",
        "  val_accuracy =[]\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print('EPOCH {}:\\n'.format(epoch + 1))\n",
        "\n",
        "    epoch_train_loss = 0\n",
        "    epoch_train_accuracy = []\n",
        "\n",
        "    epoch_val_loss = 0\n",
        "    epoch_val_accuracy = []\n",
        "\n",
        "    FC.train()\n",
        "\n",
        "    for x, y, z in dataLoaderTrain:\n",
        "\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      z = z.to(device)\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outs = FC(x, y)\n",
        "      # print(outs, z)\n",
        "      loss = loss_fn(outs, z)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_train_loss += loss.item()\n",
        "      # print(epoch_train_loss)\n",
        "\n",
        "      # print(epoch_train_loss)\n",
        "\n",
        "      outs = (outs > 0.5).float()\n",
        "      correct_predicts = (outs == z).float()\n",
        "      accuracy = ((correct_predicts.sum() / correct_predicts.size(0)) * 100)\n",
        "      epoch_train_accuracy.append(accuracy)\n",
        "\n",
        "    print('TRAIN_LOSS: {}'.format(epoch_train_loss))\n",
        "\n",
        "    avg_accuracy = (torch.stack(epoch_train_accuracy)).mean()\n",
        "    print('TRAIN_ACC: {}\\n'.format(avg_accuracy))\n",
        "\n",
        "\n",
        "    FC.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for x, y, z in dataLoaderVal:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        z = z.to(device)\n",
        "\n",
        "        outs = FC(x, y)\n",
        "        # print(outs, z)\n",
        "        # print(out.shape)\n",
        "        loss = loss_fn(outs, z)\n",
        "\n",
        "        epoch_val_loss += loss.item()\n",
        "\n",
        "        outs = (outs > 0.5).float()\n",
        "        outs = (outs == z).float()\n",
        "        accuracy = ((outs.sum() / outs.size(0)) * 100)\n",
        "        epoch_val_accuracy.append(accuracy)\n",
        "\n",
        "      print('VAL_LOSS: {}'.format(epoch_val_loss))\n",
        "\n",
        "      avg_accuracy = (torch.stack(epoch_val_accuracy)).mean()\n",
        "      print('VAL_ACC: {}\\n'.format(avg_accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a6xBjlO5Lr_",
        "outputId": "68da5de1-acf5-4fec-fa98-278e67c8525b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "\n",
            "TRAIN_LOSS: 488.87220388650894\n",
            "TRAIN_ACC: 47.54603958129883\n",
            "\n",
            "VAL_LOSS: 114.98828822374344\n",
            "VAL_ACC: 47.38429641723633\n",
            "\n",
            "EPOCH 2:\n",
            "\n",
            "TRAIN_LOSS: 446.61348700523376\n",
            "TRAIN_ACC: 47.547306060791016\n",
            "\n",
            "VAL_LOSS: 109.14435917139053\n",
            "VAL_ACC: 47.38429641723633\n",
            "\n",
            "EPOCH 3:\n",
            "\n",
            "TRAIN_LOSS: 424.8539286851883\n",
            "TRAIN_ACC: 49.00415802001953\n",
            "\n",
            "VAL_LOSS: 103.84265130758286\n",
            "VAL_ACC: 58.40169143676758\n",
            "\n",
            "EPOCH 4:\n",
            "\n",
            "TRAIN_LOSS: 403.8075992465019\n",
            "TRAIN_ACC: 69.64471435546875\n",
            "\n",
            "VAL_LOSS: 98.77258777618408\n",
            "VAL_ACC: 77.22945404052734\n",
            "\n",
            "EPOCH 5:\n",
            "\n",
            "TRAIN_LOSS: 384.0948924422264\n",
            "TRAIN_ACC: 80.87550354003906\n",
            "\n",
            "VAL_LOSS: 94.10064345598221\n",
            "VAL_ACC: 82.66936492919922\n",
            "\n",
            "EPOCH 6:\n",
            "\n",
            "TRAIN_LOSS: 366.0061659812927\n",
            "TRAIN_ACC: 83.97003173828125\n",
            "\n",
            "VAL_LOSS: 89.8560403585434\n",
            "VAL_ACC: 84.9022445678711\n",
            "\n",
            "EPOCH 7:\n",
            "\n",
            "TRAIN_LOSS: 349.52567997574806\n",
            "TRAIN_ACC: 85.7516860961914\n",
            "\n",
            "VAL_LOSS: 86.01309940218925\n",
            "VAL_ACC: 84.72108459472656\n",
            "\n",
            "EPOCH 8:\n",
            "\n",
            "TRAIN_LOSS: 334.7359192073345\n",
            "TRAIN_ACC: 85.90471649169922\n",
            "\n",
            "VAL_LOSS: 82.56414493918419\n",
            "VAL_ACC: 85.40555572509766\n",
            "\n",
            "EPOCH 9:\n",
            "\n",
            "TRAIN_LOSS: 321.5219475924969\n",
            "TRAIN_ACC: 86.15061950683594\n",
            "\n",
            "VAL_LOSS: 79.49427124857903\n",
            "VAL_ACC: 85.76274871826172\n",
            "\n",
            "EPOCH 10:\n",
            "\n",
            "TRAIN_LOSS: 309.81329783797264\n",
            "TRAIN_ACC: 86.39620971679688\n",
            "\n",
            "VAL_LOSS: 76.78259015083313\n",
            "VAL_ACC: 85.92425537109375\n",
            "\n",
            "EPOCH 11:\n",
            "\n",
            "TRAIN_LOSS: 299.42789751291275\n",
            "TRAIN_ACC: 86.46605682373047\n",
            "\n",
            "VAL_LOSS: 74.40414518117905\n",
            "VAL_ACC: 86.34126281738281\n",
            "\n",
            "EPOCH 12:\n",
            "\n",
            "TRAIN_LOSS: 290.27062514424324\n",
            "TRAIN_ACC: 86.66719818115234\n",
            "\n",
            "VAL_LOSS: 72.25592145323753\n",
            "VAL_ACC: 86.18744659423828\n",
            "\n",
            "EPOCH 13:\n",
            "\n",
            "TRAIN_LOSS: 282.08396434783936\n",
            "TRAIN_ACC: 86.65750885009766\n",
            "\n",
            "VAL_LOSS: 70.38136246800423\n",
            "VAL_ACC: 86.33699035644531\n",
            "\n",
            "EPOCH 14:\n",
            "\n",
            "TRAIN_LOSS: 274.75529459118843\n",
            "TRAIN_ACC: 86.74609375\n",
            "\n",
            "VAL_LOSS: 68.6689703464508\n",
            "VAL_ACC: 86.37202453613281\n",
            "\n",
            "EPOCH 15:\n",
            "\n",
            "TRAIN_LOSS: 268.15813487768173\n",
            "TRAIN_ACC: 86.67719268798828\n",
            "\n",
            "VAL_LOSS: 67.15327560901642\n",
            "VAL_ACC: 86.50191497802734\n",
            "\n",
            "EPOCH 16:\n",
            "\n",
            "TRAIN_LOSS: 262.2235679924488\n",
            "TRAIN_ACC: 86.75609588623047\n",
            "\n",
            "VAL_LOSS: 65.79674431681633\n",
            "VAL_ACC: 86.42329406738281\n",
            "\n",
            "EPOCH 17:\n",
            "\n",
            "TRAIN_LOSS: 257.04846289753914\n",
            "TRAIN_ACC: 86.73625183105469\n",
            "\n",
            "VAL_LOSS: 64.65642359852791\n",
            "VAL_ACC: 86.64803314208984\n",
            "\n",
            "EPOCH 18:\n",
            "\n",
            "TRAIN_LOSS: 252.49467766284943\n",
            "TRAIN_ACC: 86.74625396728516\n",
            "\n",
            "VAL_LOSS: 63.583222568035126\n",
            "VAL_ACC: 86.72237396240234\n",
            "\n",
            "EPOCH 19:\n",
            "\n",
            "TRAIN_LOSS: 248.34969061613083\n",
            "TRAIN_ACC: 86.7710189819336\n",
            "\n",
            "VAL_LOSS: 62.62300366163254\n",
            "VAL_ACC: 86.73776245117188\n",
            "\n",
            "EPOCH 20:\n",
            "\n",
            "TRAIN_LOSS: 244.62834905087948\n",
            "TRAIN_ACC: 86.75133514404297\n",
            "\n",
            "VAL_LOSS: 61.74494016170502\n",
            "VAL_ACC: 86.65914916992188\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "# lr_transformer = 0.001\n",
        "lr_transformer = 0.001\n",
        "BinaryClassifier_Trainer(BCTrainLoader, BCValLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq6_7rh48bQA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXB1OsX3ILbJ"
      },
      "source": [
        "# PLOTTING THE DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_AM-XPYHYjd",
        "outputId": "11a20590-eeef-4fb3-df18-9290861b1dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20319 20319\n"
          ]
        }
      ],
      "source": [
        "# lf = nn.BCELoss()\n",
        "\n",
        "# for x, y, z in BCTrainLoader:\n",
        "#   x = x.to(device)\n",
        "#   y = y.to(device)\n",
        "#   z = z.to(device)\n",
        "#   out = BC(x, y)\n",
        "#   print(lf(out, z))\n",
        "#   break\n",
        "scatter_values = []\n",
        "scatter_labels = []\n",
        "\n",
        "test_lf = mse_loss_new_withoutBatchAverage()\n",
        "test_sigmoid = nn.Sigmoid()\n",
        "test_linear = nn.Linear(1,1)\n",
        "test_linear = test_linear.to(device)\n",
        "for x, y, z in BCTrainLoader:\n",
        "  test_loss = test_lf(x,y)\n",
        "  # outs = test_linear(test_loss)\n",
        "  # outs = test_sigmoid(outs)\n",
        "  outs = test_sigmoid(test_loss)\n",
        "  # test_loss = test_loss.tolist()\n",
        "  outs = outs.tolist()\n",
        "  z = z.tolist()\n",
        "  # scatter_values = scatter_values + test_loss\n",
        "  scatter_values = scatter_values + outs\n",
        "  scatter_labels = scatter_labels + z\n",
        "  # print(len(test_loss), len(z))\n",
        "\n",
        "scatter_values = [value[0] for value in scatter_values]\n",
        "scatter_labels = [value[0] for value in scatter_labels]\n",
        "\n",
        "\n",
        "print(len(scatter_values), len(scatter_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-KUOsfeXVdC",
        "outputId": "6c38bdad-8fab-4a3f-a607-7e2513cd450d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'float'>\n",
            "0.816306471824646\n"
          ]
        }
      ],
      "source": [
        "print(type(scatter_values[0]))\n",
        "print(scatter_values[20318])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "rGmccT8PQ3um",
        "outputId": "d37bd42d-f8a6-40f7-925d-177556b2ed81"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "\n",
        "# Define colors for labels\n",
        "colors = ['blue' if label == 0 else 'red' for label in scatter_labels]\n",
        "plt.scatter(x= scatter_values, y=[0]*len(scatter_values) , c=colors, marker='o')\n",
        "\n",
        "plt.xlabel('Loss Values')\n",
        "plt.yticks()  # Hide y-axis ticks and labels\n",
        "plt.title('Fake vs Real Distribution')\n",
        "# plt.colorbar(label='Labels')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vLCzKlUkbaiw",
        "VOMIkzfmyNQ5",
        "sD7H-NB3bVle",
        "z_aPhDHRcGrf",
        "TJcE9LXIA7_M",
        "AqRB4l7iswlQ",
        "h5hQywM2IB9h"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
